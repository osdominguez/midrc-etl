{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import PosixPath\n",
    "import pandas as pd\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = random.Random()\n",
    "rnd.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = [\n",
    "    # \"packages_acrimage/2021/06\",\n",
    "    # \"packages_acrimage/2021/07\",\n",
    "    # \"packages_acrimage/2021/08\",\n",
    "    # \"packages_acrimage/2021/0827\",\n",
    "    # \"packages_acrimage/PETAL\",\n",
    "    # \"packages_acrimage/2021/09\",\n",
    "    # \"packages_acrimage/2021/10/batch6\",\n",
    "    # \"packages_acrimage/2021/10/batch7\",\n",
    "    # \"packages_acrimage/2021/11\",\n",
    "    # \"packages_acrimage/2021/ACRPETAL_20211220\",\n",
    "    # \"packages_acrimage/2022/ACR_20220107\",\n",
    "    # \"ACR_20211115\",\n",
    "    # \"ACR_20220107\",\n",
    "    # \"packages_ACR_20220218\",\n",
    "]\n",
    "\n",
    "submission_path = PosixPath(\n",
    "    \"~/CTDS/projects/midrc/indexing-data/packages_acr\"\n",
    ").expanduser()\n",
    "\n",
    "to_index_path = submission_path / \"..\" / \"to_index_acr\"\n",
    "(to_index_path/\"open\").mkdir(parents=True, exist_ok=True)\n",
    "(to_index_path/\"seq\").mkdir(parents=True, exist_ok=True)\n",
    "(to_index_path/\"remove\").mkdir(parents=True, exist_ok=True)\n",
    "(to_index_path/\"missing\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sequestration_master_file_path = PosixPath(\n",
    "    \"~/CTDS/projects/midrc/indexing-data/sequestration/master_sequestration_locations_16188_2022-04-13.tsv\"\n",
    ").expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = PosixPath(\"~/CTDS/projects/midrc/processed-s3/acrimage\").expanduser()\n",
    "\n",
    "# for RemoveHeads files, they have the same structure and needs to remove imaging_study\n",
    "# which is `submitter_id` in the format of\n",
    "# <case>_<study_id>\n",
    "remove_heads_files = RAW_DATA.glob(\"**/RemoveHeads*.txt\")\n",
    "remove_heads_studies = map(lambda v: pd.read_csv(v, sep=\"\\t\"), remove_heads_files)\n",
    "remove_heads_studies = map(lambda v: v[\"submitter_id\"] \\\n",
    "        .str.split(\"_\", expand=True) \\\n",
    "        .rename(columns={0: \"case_id\", 1: \"study_id\"})[[\"study_id\"]],\n",
    "    remove_heads_studies)\n",
    "remove_heads_studies = pd.concat(remove_heads_studies).reset_index(drop=True)\n",
    "\n",
    "# same thing for different format of deletion files\n",
    "# there are two different formats: one for imaging_study and one for images :facepalm:\n",
    "# this needs some column renaming\n",
    "rename_columns = {\n",
    "    \"*type\": \"type\",\n",
    "    \"*submitter_id\": \"submitter_id\",\n",
    "    \"study_uid\": \"study_id\",\n",
    "}\n",
    "\n",
    "deletion_imaging_study_files = RAW_DATA.glob(\"**/deletion_*.tsv\")\n",
    "deletion_imaging_study_studies = map(lambda v: pd.read_csv(v, sep=\"\\t\") \\\n",
    "        .rename(columns=rename_columns),\n",
    "    deletion_imaging_study_files)\n",
    "deletion_imaging_study_studies = map(lambda v: v[[\"study_id\"]], deletion_imaging_study_studies)\n",
    "deletion_imaging_study_studies = pd.concat(deletion_imaging_study_studies).reset_index(drop=True)\n",
    "\n",
    "studies_to_delete = pd.concat([remove_heads_studies, deletion_imaging_study_studies]).reset_index(drop=True)\n",
    "match_studies_to_delete = studies_to_delete[\"study_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_master = {}\n",
    "with open(sequestration_master_file_path) as sequestration_master_file:\n",
    "    reader = csv.DictReader(sequestration_master_file, delimiter=\"\\t\")\n",
    "\n",
    "    for row in reader:\n",
    "        seq_master[row[\"case_ids\"]] = row[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in submissions:\n",
    "    package_files = submission_path / submission / \"packages\"\n",
    "\n",
    "    print(package_files)\n",
    "\n",
    "    open_packages = []\n",
    "    seq_packages = []\n",
    "    to_remove_packages = []\n",
    "    missing_packages = []\n",
    "\n",
    "    for package_filepath in package_files.iterdir():\n",
    "        with open(package_filepath) as package_file:\n",
    "            reader = csv.DictReader(package_file, delimiter=\"\\t\")\n",
    "            for row in reader:\n",
    "                item = row\n",
    "\n",
    "                file_name = item[\"file_name\"]\n",
    "                case_id, study_id, _ = file_name.split(\"/\")\n",
    "\n",
    "                # in case study_id have some prefixes\n",
    "                study_id = study_id.split(\"_\")[-1]\n",
    "\n",
    "                package_contents = json.loads(item[\"package_contents\"].replace(\"'\", \"\\\"\"))\n",
    "                for p in package_contents:\n",
    "                    p[\"size\"] = int(p[\"size\"])\n",
    "                \n",
    "                item[\"package_contents\"] = json.dumps(package_contents)\n",
    "\n",
    "                dataset = seq_master.get(case_id, None)\n",
    "\n",
    "                if dataset == \"Open\":\n",
    "                    bucket = \"s3://open-data-midrc/\"\n",
    "                    authz = json.dumps([\"/programs/Open/projects/A1\"])\n",
    "                elif dataset == \"Seq\":\n",
    "                    bucket = \"s3://sequestered-data-midrc/\"\n",
    "                    authz = json.dumps([\"/programs/SEQ_Open/projects/A3\"])\n",
    "                else:\n",
    "                    authz = \"\"\n",
    "                    bucket = \"\"\n",
    "\n",
    "                item[\"authz\"] = authz\n",
    "                item[\"url\"] = f\"{bucket}{item['url']}\"\n",
    "\n",
    "                if study_id in match_studies_to_delete:\n",
    "                    to_remove_packages.append(item)\n",
    "                    continue\n",
    "\n",
    "                m = hashlib.md5()\n",
    "                m.update(f\"{item['md5']}{item['size']}\".encode('utf-8'))\n",
    "                item[\"guid\"] = f\"dg.MD1R/{uuid.UUID(m.hexdigest(), version=4)}\"\n",
    "                if dataset == \"Open\":\n",
    "                    open_packages.append(item)\n",
    "                elif dataset == \"Seq\":\n",
    "                    seq_packages.append(item)\n",
    "                else:\n",
    "                    missing_packages.append(item)\n",
    "\n",
    "    datasets = [\n",
    "        (f\"open/new_packages_open_{submission.split('/')[-1].removeprefix('packages_')}.tsv\", open_packages),\n",
    "        (f\"seq/new_packages_seq_{submission.split('/')[-1].removeprefix('packages_')}.tsv\", seq_packages),\n",
    "        (f\"remove/new_packages_remove_{submission.split('/')[-1].removeprefix('packages_')}.tsv\", to_remove_packages),\n",
    "        (f\"missing/new_packages_missing_{submission.split('/')[-1].removeprefix('packages_')}.tsv\", missing_packages),\n",
    "    ]\n",
    "\n",
    "    fieldnames = [\n",
    "        \"record_type\",\n",
    "        \"guid\",\n",
    "        \"md5\",\n",
    "        \"size\",\n",
    "        \"authz\",\n",
    "        \"url\",\n",
    "        \"file_name\",\n",
    "        \"package_contents\",\n",
    "    ]\n",
    "\n",
    "    for filename, dataset in datasets:\n",
    "        if not dataset:\n",
    "            continue\n",
    "        with open(\n",
    "            to_index_path / filename,\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for item in dataset:\n",
    "                writer.writerow(item)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5c72d1243d6df0c85fac95eab025b92ad6f1709943deea63d571be8293a3eb7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit ('midrc-etl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
